{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eeec391-d65f-48f0-9176-7cf49bfbde86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23c238-6728-47d3-a5ad-0647042bc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports & paths\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "# Try to use transforms.v2 (faster C++ backend); fallback to classic transforms\n",
    "try:\n",
    "    from torchvision.transforms import v2 as T\n",
    "    HAVE_V2 = True\n",
    "    print(\"Using torchvision.transforms.v2\")\n",
    "except ImportError:\n",
    "    from torchvision import transforms as T\n",
    "    HAVE_V2 = False\n",
    "    print(\"Using torchvision.transforms (legacy)\")\n",
    "\n",
    "# ---- Paths ----\n",
    "REPO_ROOT = Path.cwd().parents[0] if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "RAW_ROOT = REPO_ROOT / \"data\" / \"raw\"      # original yolo-style\n",
    "SW_PATCH_ROOT = REPO_ROOT / \"data\" / \"patches_sw\"  # sliding-window patches\n",
    "\n",
    "print(\"Repo root:\", REPO_ROOT)\n",
    "print(\"Sliding-window patch root:\", SW_PATCH_ROOT)\n",
    "\n",
    "# ---- GPU & multiprocessing setup ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(\"CUDA:\", torch.cuda.is_available())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# On Windows + Jupyter: needed for num_workers > 0\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "print(\"Multiprocessing start method set to 'spawn'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbbbb4ca-bdf9-4f85-b5f2-b68928af658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root:     C:\\Users\\amrik\\Data Science\\Personal-PCB-Project\n",
      "SW_PATCH_ROOT: C:\\Users\\amrik\\Data Science\\Personal-PCB-Project\\data\\patches_sw\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ed724b-0320-46fa-8d39-fe8d4361d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yolo_labels(label_path, img_w, img_h):\n",
    "    \"\"\"\n",
    "    Return a list of (cls_idx, x1, y1, x2, y2) in PIXELS from a YOLO txt file.\n",
    "    YOLO format per line: cls cx cy w h (all normalized 0â€“1).\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    if not label_path.exists():\n",
    "        return boxes\n",
    "\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "            cls = int(parts[0])\n",
    "            cx, cy, bw, bh = map(float, parts[1:])\n",
    "\n",
    "            # convert to pixel coords\n",
    "            cx *= img_w\n",
    "            cy *= img_h\n",
    "            bw *= img_w\n",
    "            bh *= img_h\n",
    "\n",
    "            x1 = cx - bw / 2\n",
    "            y1 = cy - bh / 2\n",
    "            x2 = cx + bw / 2\n",
    "            y2 = cy + bh / 2\n",
    "\n",
    "            # clip\n",
    "            x1 = max(0, min(img_w - 1, x1))\n",
    "            y1 = max(0, min(img_h - 1, y1))\n",
    "            x2 = max(0, min(img_w - 1, x2))\n",
    "            y2 = max(0, min(img_h - 1, y2))\n",
    "\n",
    "            boxes.append((cls, x1, y1, x2, y2))\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8450ad26-ed96-4345-9d16-de045c9450b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou_xyxy(box_a, box_b):\n",
    "    \"\"\"IoU between box_a and box_b, each = (x1,y1,x2,y2).\"\"\"\n",
    "    xa1, ya1, xa2, ya2 = box_a\n",
    "    xb1, yb1, xb2, yb2 = box_b\n",
    "\n",
    "    inter_x1 = max(xa1, xb1)\n",
    "    inter_y1 = max(ya1, yb1)\n",
    "    inter_x2 = min(xa2, xb2)\n",
    "    inter_y2 = min(ya2, yb2)\n",
    "\n",
    "    if inter_x2 <= inter_x1 or inter_y2 <= inter_y1:\n",
    "        return 0.0\n",
    "\n",
    "    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)\n",
    "    area_a = (xa2 - xa1) * (ya2 - ya1)\n",
    "    area_b = (xb2 - xb1) * (yb2 - yb1)\n",
    "\n",
    "    return inter_area / (area_a + area_b - inter_area + 1e-9)\n",
    "\n",
    "\n",
    "def label_window_overlap(x1, y1, x2, y2, gt_boxes, min_rel_overlap=0.30):\n",
    "    \"\"\"\n",
    "    Label a sliding window based on how much it overlaps any ground-truth box.\n",
    "\n",
    "    - gt_boxes: list of (cls_idx, gx1, gy1, gx2, gy2) in pixels.\n",
    "    - min_rel_overlap: minimum (intersection_area / box_area) to mark as positive.\n",
    "\n",
    "    Returns: class_name (\"missing_hole\", ..., \"background\").\n",
    "    \"\"\"\n",
    "\n",
    "    best_cls = None\n",
    "    best_ratio = 0.0\n",
    "\n",
    "    for (cls_idx, gx1, gy1, gx2, gy2) in gt_boxes:\n",
    "        # intersection\n",
    "        ix1 = max(x1, gx1)\n",
    "        iy1 = max(y1, gy1)\n",
    "        ix2 = min(x2, gx2)\n",
    "        iy2 = min(y2, gy2)\n",
    "\n",
    "        if ix2 <= ix1 or iy2 <= iy1:\n",
    "            continue  # no overlap\n",
    "\n",
    "        inter_area = (ix2 - ix1) * (iy2 - iy1)\n",
    "        box_area   = (gx2 - gx1) * (gy2 - gy1)\n",
    "        if box_area <= 0:\n",
    "            continue\n",
    "\n",
    "        # how much of the *defect box* is covered by this window?\n",
    "        rel_overlap = inter_area / box_area\n",
    "\n",
    "        if rel_overlap > best_ratio:\n",
    "            best_ratio = rel_overlap\n",
    "            best_cls = DEFECT_CLASSES[cls_idx]\n",
    "\n",
    "    if best_cls is not None and best_ratio >= min_rel_overlap:\n",
    "        return best_cls\n",
    "\n",
    "    return \"background\"\n",
    "\n",
    "\n",
    "\n",
    "def label_window_center(x1, y1, x2, y2, gt_boxes):\n",
    "    \"\"\"\n",
    "    Label a sliding window based on whether its CENTER lies inside a ground-truth box.\n",
    "    gt_boxes: list of (cls_idx, gx1, gy1, gx2, gy2) in pixels.\n",
    "\n",
    "    Returns: class_name (\"missing_hole\", ..., \"background\") or None (if we ever want to ignore).\n",
    "    \"\"\"\n",
    "    cx = 0.5 * (x1 + x2)\n",
    "    cy = 0.5 * (y1 + y2)\n",
    "\n",
    "    if not gt_boxes:\n",
    "        # no defects in this image at all\n",
    "        return \"background\"\n",
    "\n",
    "    for (cls_idx, gx1, gy1, gx2, gy2) in gt_boxes:\n",
    "        if (gx1 <= cx <= gx2) and (gy1 <= cy <= gy2):\n",
    "            # center lies inside this defect bbox\n",
    "            return DEFECT_CLASSES[cls_idx]   # YOLO cls_idx is 0..5\n",
    "\n",
    "    # center doesnâ€™t fall inside any bbox\n",
    "    return \"background\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84504386-ab25-479e-89e4-7dccb72259c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patches_for_image(img_path, split):\n",
    "    \"\"\"\n",
    "    Slide window over single image, label each patch via CENTER-IN-BOX rule, save cropped patch.\n",
    "    Returns: (num_pos, num_bg)\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    W, H = img.size\n",
    "\n",
    "    # label file path (filenames match exactly in your dataset)\n",
    "    label_name = img_path.stem + \".txt\"\n",
    "    label_path = img_path.parent.parent / \"labels\" / label_name\n",
    "\n",
    "    gt_boxes = read_yolo_labels(label_path, W, H)\n",
    "\n",
    "    num_pos = 0\n",
    "    num_bg = 0\n",
    "\n",
    "    base_name = img_path.stem\n",
    "\n",
    "    for y in range(0, H - PATCH_SIZE + 1, STRIDE):\n",
    "        for x in range(0, W - PATCH_SIZE + 1, STRIDE):\n",
    "            x1, y1 = x, y\n",
    "            x2, y2 = x + PATCH_SIZE, y + PATCH_SIZE\n",
    "\n",
    "            cls_name = label_window_center(x1, y1, x2, y2, gt_boxes)\n",
    "\n",
    "            # optional: subsample background so it doesnâ€™t dominate too hard\n",
    "            if cls_name == \"background\" and np.random.rand() > BG_KEEP_PROB:\n",
    "                continue\n",
    "\n",
    "            patch = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "            out_dir = SW_PATCH_ROOT / split / cls_name\n",
    "            out_name = f\"{base_name}_x{x}_y{y}.jpg\"\n",
    "            patch.save(out_dir / out_name, quality=95)\n",
    "\n",
    "            if cls_name == \"background\":\n",
    "                num_bg += 1\n",
    "            else:\n",
    "                num_pos += 1\n",
    "\n",
    "    return num_pos, num_bg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac12750-824e-4ee3-b479-1322bf48b8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old sliding-window patches at: C:\\Users\\amrik\\Data Science\\Personal-PCB-Project\\data\\patches_sw\n",
      "Fresh SW_PATCH_ROOT created: C:\\Users\\amrik\\Data Science\\Personal-PCB-Project\\data\\patches_sw\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "if SW_PATCH_ROOT.exists():\n",
    "    print(\"Removing old sliding-window patches at:\", SW_PATCH_ROOT)\n",
    "    shutil.rmtree(SW_PATCH_ROOT)\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    for cls in ALL_CLASSES:\n",
    "        out_dir = SW_PATCH_ROOT / split / cls\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Fresh SW_PATCH_ROOT created:\", SW_PATCH_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f7b9a16-d25d-4b88-bb6b-90545f7977e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patches_for_image(img_path, split):\n",
    "    \"\"\"\n",
    "    Slide window over single image, label each patch via overlap-with-box rule, save cropped patch.\n",
    "    Returns: (num_pos, num_bg)\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    W, H = img.size\n",
    "\n",
    "    # label file path\n",
    "    label_path = img_path.parent.parent / \"labels\" / (img_path.stem + \".txt\")\n",
    "\n",
    "    if not label_path.exists():\n",
    "        print(f\"[WARN] No label file for: {img_path.name}\")\n",
    "        gt_boxes = []\n",
    "    else:\n",
    "        gt_boxes = read_yolo_labels(label_path, W, H)\n",
    "\n",
    "\n",
    "    num_pos = 0\n",
    "    num_bg = 0\n",
    "\n",
    "    base_name = img_path.stem\n",
    "\n",
    "    for y in range(0, H - PATCH_SIZE + 1, STRIDE):\n",
    "        for x in range(0, W - PATCH_SIZE + 1, STRIDE):\n",
    "            x1, y1 = x, y\n",
    "            x2, y2 = x + PATCH_SIZE, y + PATCH_SIZE\n",
    "\n",
    "            cls_name = label_window_overlap(x1, y1, x2, y2, gt_boxes,\n",
    "                                            min_rel_overlap=0.30)\n",
    "\n",
    "            # background subsampling\n",
    "            if cls_name == \"background\" and np.random.rand() > BG_KEEP_PROB:\n",
    "                continue\n",
    "\n",
    "            patch = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "            out_dir = SW_PATCH_ROOT / split / cls_name\n",
    "            out_name = f\"{base_name}_x{x}_y{y}.jpg\"\n",
    "            patch.save(out_dir / out_name, quality=95)\n",
    "\n",
    "            if cls_name == \"background\":\n",
    "                num_bg += 1\n",
    "            else:\n",
    "                num_pos += 1\n",
    "\n",
    "    return num_pos, num_bg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141ad8b7-78d8-4633-8722-e037715d15b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image: 01_missing_hole_01_jpg.rf.048b220b403c2678f1a9f40f5bb3a8eb.jpg\n",
      "From this single image -> positives: 3 background: 7\n"
     ]
    }
   ],
   "source": [
    "train_img_dir = RAW_ROOT / \"train\" / \"images\"\n",
    "one_img = sorted(train_img_dir.glob(\"*.jpg\"))[0]\n",
    "print(\"Test image:\", one_img.name)\n",
    "\n",
    "p, b = generate_patches_for_image(one_img, split=\"train\")\n",
    "print(\"From this single image -> positives:\", p, \"background:\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277e8866-13a1-4a5a-b3f7-29130152404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " train\n",
      "  background      -> 24354\n",
      "  missing_hole    -> 3988\n",
      "  mouse_bite      -> 4120\n",
      "  open_circuit    -> 3771\n",
      "  short           -> 4032\n",
      "  spur            -> 3971\n",
      "  spurious_copper -> 3382\n",
      "\n",
      " valid\n",
      "  background      -> 11306\n",
      "  missing_hole    -> 1871\n",
      "  mouse_bite      -> 2140\n",
      "  open_circuit    -> 1846\n",
      "  short           -> 2169\n",
      "  spur            -> 1863\n",
      "  spurious_copper -> 1753\n",
      "\n",
      " test\n",
      "  background      -> 3866\n",
      "  missing_hole    -> 715\n",
      "  mouse_bite      -> 460\n",
      "  open_circuit    -> 674\n",
      "  short           -> 840\n",
      "  spur            -> 610\n",
      "  spurious_copper -> 622\n"
     ]
    }
   ],
   "source": [
    "PRINT_ROOT = SW_PATCH_ROOT\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    print(\"\\n\", split)\n",
    "    split_dir = PRINT_ROOT / split\n",
    "    for cls in sorted(os.listdir(split_dir)):\n",
    "        cls_dir = split_dir / cls\n",
    "        if cls_dir.is_dir():\n",
    "            n = len(list(cls_dir.glob(\"*.jpg\")))\n",
    "            print(f\"  {cls:15s} -> {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7493e2fa-5581-4adb-af9e-cf9b006dc7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating sliding-window patches for train (3224 images) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3224/3224 [02:41<00:00, 20.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train: positives=23219, background_kept=23125\n",
      "\n",
      "=== Generating sliding-window patches for valid (1592 images) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1592/1592 [01:21<00:00, 19.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split valid: positives=11642, background_kept=11306\n",
      "\n",
      "=== Generating sliding-window patches for test (537 images) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 537/537 [00:19<00:00, 27.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split test: positives=3921, background_kept=3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_all_patches_for_split(split):\n",
    "    img_dir = RAW_ROOT / split / \"images\"\n",
    "    img_paths = sorted(img_dir.glob(\"*.jpg\"))\n",
    "\n",
    "    total_pos = 0\n",
    "    total_bg = 0\n",
    "\n",
    "    print(f\"\\n=== Generating sliding-window patches for {split} \"\n",
    "          f\"({len(img_paths)} images) ===\")\n",
    "    for img_path in tqdm(img_paths):\n",
    "        p, b = generate_patches_for_image(img_path, split)\n",
    "        total_pos += p\n",
    "        total_bg += b\n",
    "\n",
    "    print(f\"Split {split}: positives={total_pos}, background_kept={total_bg}\")\n",
    "\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    generate_all_patches_for_split(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e15112-c5d8-41a5-bfc5-912314083f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 47618\n",
      "Valid size: 22948\n",
      "Test  size: 7787\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Valid size:\", len(val_dataset))\n",
    "print(\"Test  size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59cade9e-bb42-4562-b31c-941835ebf5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: hyperparameters & transforms\n",
    "\n",
    "IMG_SIZE = 160\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4          # tune if CPU has more/less cores\n",
    "PREFETCH_FACTOR = 2\n",
    "PIN_MEMORY = (device.type == \"cuda\")\n",
    "\n",
    "EPOCHS = 8               # you can bump later\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# mean/std for PCB-like green boards (rough approximation; not critical)\n",
    "MEAN = (0.5, 0.5, 0.5)\n",
    "STD  = (0.25, 0.25, 0.25)\n",
    "\n",
    "if HAVE_V2:\n",
    "    train_transform = T.Compose([\n",
    "        T.ToImage(),                           # faster tensor conversion\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomVerticalFlip(p=0.5),\n",
    "        T.RandomRotation(degrees=5),\n",
    "        T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(MEAN, STD),\n",
    "    ])\n",
    "\n",
    "    eval_transform = T.Compose([\n",
    "        T.ToImage(),\n",
    "        T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(MEAN, STD),\n",
    "    ])\n",
    "else:\n",
    "    train_transform = T.Compose([\n",
    "        T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomVerticalFlip(p=0.5),\n",
    "        T.RandomRotation(degrees=5),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(MEAN, STD),\n",
    "    ])\n",
    "\n",
    "    eval_transform = T.Compose([\n",
    "        T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(MEAN, STD),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9418e178-e6c5-41e1-874d-80fb0edc9bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['background', 'missing_hole', 'mouse_bite', 'open_circuit', 'short', 'spur', 'spurious_copper']\n",
      "Train size: 47618\n",
      "Valid size: 22948\n",
      "Test  size: 7787\n",
      "Batches -> train: 373 valid: 180 test: 61\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: datasets and dataloaders\n",
    "\n",
    "train_dir = SW_PATCH_ROOT / \"train\"\n",
    "val_dir   = SW_PATCH_ROOT / \"valid\"\n",
    "test_dir  = SW_PATCH_ROOT / \"test\"\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_dataset   = datasets.ImageFolder(val_dir,   transform=eval_transform)\n",
    "test_dataset  = datasets.ImageFolder(test_dir,  transform=eval_transform)\n",
    "\n",
    "classes = train_dataset.classes\n",
    "num_classes = len(classes)\n",
    "\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Valid size:\", len(val_dataset))\n",
    "print(\"Test  size:\", len(test_dataset))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=PREFETCH_FACTOR,\n",
    ")\n",
    "\n",
    "print(\"Batches -> train:\", len(train_loader), \n",
    "      \"valid:\", len(val_loader), \"test:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee5970ac-7daa-4d12-91fc-571963f7b069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amrik\\AppData\\Local\\Temp\\ipykernel_34068\\2264009423.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == \"cuda\"))\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: model, loss, optimizer, scheduler, scaler\n",
    "\n",
    "from torchvision import models\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# Pretrained MobileNetV2 backbone\n",
    "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Replace classifier head\n",
    "in_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    ")\n",
    "\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "print(\"Model ready on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd71e271-6a3f-4ff5-a254-30eecd2d4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: train/test epoch functions (mixed precision + timing)\n",
    "\n",
    "from torch.amp import autocast  # new API (Torch 2.x)\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, scaler, device, epoch=None, total_epochs=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # ---> Progress bar for batches\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{total_epochs} [Train]\", leave=False)\n",
    "\n",
    "    for inputs, targets in pbar:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type==\"cuda\")):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # stats update\n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        _, preds = outputs.max(1)\n",
    "        running_correct += preds.eq(targets).sum().item()\n",
    "        total += batch_size\n",
    "\n",
    "        # update tqdm bar display\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{running_loss/total:.4f}\",\n",
    "            \"acc\": f\"{running_correct/total:.4f}\"\n",
    "        })\n",
    "\n",
    "    return running_loss / total, running_correct / total\n",
    "\n",
    "\n",
    "def eval_one_epoch(model, dataloader, criterion, device, epoch=None, total_epochs=None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{total_epochs} [Eval]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in pbar:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type==\"cuda\")):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            _, preds = outputs.max(1)\n",
    "            running_correct += preds.eq(targets).sum().item()\n",
    "            total += batch_size\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{running_loss/total:.4f}\",\n",
    "                \"acc\": f\"{running_correct/total:.4f}\"\n",
    "            })\n",
    "\n",
    "    return running_loss / total, running_correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b918579-a2d4-4797-9eb1-d12ce3f8678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 8 epochs on cuda\n",
      "\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1933, Train acc: 0.6058\n",
      "Val   loss: 0.9483, Val   acc: 0.6957\n",
      "  ðŸ”¥ New best model saved!\n",
      "\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7082, Train acc: 0.7821\n",
      "Val   loss: 0.5541, Val   acc: 0.8433\n",
      "  ðŸ”¥ New best model saved!\n",
      "\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4924, Train acc: 0.8600\n",
      "Val   loss: 0.4682, Val   acc: 0.8715\n",
      "  ðŸ”¥ New best model saved!\n",
      "\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4378, Train acc: 0.8794\n",
      "Val   loss: 0.3992, Val   acc: 0.8916\n",
      "  ðŸ”¥ New best model saved!\n",
      "\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3895, Train acc: 0.8937\n",
      "Val   loss: 0.3541, Val   acc: 0.9075\n",
      "  ðŸ”¥ New best model saved!\n",
      "\n",
      "Epoch 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3537, Train acc: 0.9046\n",
      "Val   loss: 0.3442, Val   acc: 0.9082\n",
      "  ðŸ”¥ New best model saved!\n",
      "\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3638, Train acc: 0.9041\n",
      "Val   loss: 0.3128, Val   acc: 0.9191\n",
      "  ðŸ”¥ New best model saved!\n",
      "\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3308, Train acc: 0.9126\n",
      "Val   loss: 0.3138, Val   acc: 0.9195\n",
      "  ðŸ”¥ New best model saved!\n",
      "\n",
      "Training done.\n",
      "Best validation accuracy: 0.9195136831096392\n",
      "Best model path: C:\\Users\\amrik\\Data Science\\Personal-PCB-Project\\results\\mobilenetv2_sw_best.pth\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: main training loop\n",
    "\n",
    "RESULTS_DIR = REPO_ROOT / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "best_path = RESULTS_DIR / \"mobilenetv2_sw_best.pth\"\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs on\", device)\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    # ---- Train ----\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, scaler, device\n",
    "    )\n",
    "\n",
    "    # ---- Validate ----\n",
    "    val_loss, val_acc = eval_one_epoch(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    # LR scheduler on validation accuracy\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   loss: {val_loss:.4f}, Val   acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(\"  ðŸ”¥ New best model saved!\")\n",
    "\n",
    "print(\"\\nTraining done.\")\n",
    "print(\"Best validation accuracy:\", best_val_acc)\n",
    "print(\"Best model path:\", best_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aadc2b6-6621-4481-a49d-69780e479950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcb-defect (PyTorch)",
   "language": "python",
   "name": "pcb-defect"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
